{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New notebook to keep dev clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import praw\n",
    "from praw import Reddit\n",
    "\n",
    "class WallpaperScraper:\n",
    "    reddit_oath = {\n",
    "        'client_id': 'pIxpnAoiGfwE-g',\n",
    "        'client_secret': 'Ueccpv4dJegXUYbmAIdwxevxjDs',\n",
    "        'user_agent': 'wallpapers'\n",
    "    }\n",
    "    \n",
    "    imgur_oath = {\n",
    "        'client_id': '29b27fc1363aa92',\n",
    "        'client_secret': 'ef58b651b764dfb487809e64a2d62982e1a6392e'\n",
    "    }\n",
    "    \n",
    "    imgur_headers = {\n",
    "      'Authorization': f'Client-ID {imgur_oath[\"client_id\"]}'\n",
    "    }\n",
    "    \n",
    "#     sort_options = {'controversial', 'gilded', 'hot', 'new', 'rising', 'top'}\n",
    "#     time_filter = {'all', 'day', 'hour', 'month', 'week', 'year'}\n",
    "    \n",
    "    def __init__(self, img_dir, sort='top', time_filter='week'):\n",
    "        \"\"\"\n",
    "        Initialize a wallpaper subreddit scraper instance with feed options.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._img_dir = img_dir\n",
    "        self._mk_img_dir()\n",
    "        \n",
    "        self._wallpapers = Reddit(**self.reddit_oath).subreddit('wallpapers')\n",
    "        if sort in {'controversial', 'top'}:\n",
    "            self._wallpapers = getattr(self._wallpapers, sort)(limit=1000, time_filter=time_filter)\n",
    "        else:\n",
    "            self._wallpapers = getattr(self._wallpapers, sort)(limit=1000)\n",
    "        \n",
    "    def _mk_img_dir(self):\n",
    "        \"\"\"\n",
    "        Checks and initializes a directory for image files to be downloaded.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self._img_dir):\n",
    "            os.mkdir(self._img_dir)\n",
    "        if not os.path.exists(os.path.join(self._img_dir, 'buffer')):\n",
    "            os.mkdir(os.path.join(self._img_dir, 'buffer'))\n",
    "            \n",
    "    def _img_url_extract(self, img_sub):\n",
    "        \"\"\"\n",
    "        Extract and return the url hyperlink of only PRAW's submission-type objects.\n",
    "        \"\"\"\n",
    "        if isinstance(img_sub, praw.models.reddit.submission.Submission):\n",
    "            return img_sub.url\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = Reddit(**WallpaperScraper.reddit_oath).subreddit('wallpapers')\n",
    "\n",
    "# Time filters only allowed for 'controversial' and 'top' sort orders\n",
    "\n",
    "for i in ['controversial', 'gilded', 'hot', 'new', 'rising', 'top']:\n",
    "    for j in ['all', 'day', 'hour', 'month', 'week', 'year']:\n",
    "        try:\n",
    "            getattr(test, i)(time_filter=j)\n",
    "        except:\n",
    "            print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WallpaperScraper('wallpapers', 'top', 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, img_sub in enumerate(test._wallpapers):\n",
    "    if isinstance(img_sub, praw.models.reddit.submission.Submission):\n",
    "        img_sub.url.split('.')[-1]\n",
    "        file_ext = img_sub.url.split('.')[-1]\n",
    "        print(img_sub.url, file_ext, img_sub.thumbnail)\n",
    "    #     with open(f'tmp/tmp{idx}.{file_ext}', 'wb') as tmp:\n",
    "    #         r = requests.get(img_sub.url, stream=True)\n",
    "    #         if r.ok:\n",
    "    #             tmp.write(r.content)\n",
    "    #     file_ext = img_sub.thumbnail.split('.')[-1]\n",
    "    #     with open(f'tmp/tmp{idx}_.{file_ext}', 'wb') as tmp:\n",
    "    #         r = requests.get(img_sub.thumbnail, stream=True)\n",
    "    #         if r.ok:\n",
    "    #             tmp.write(r.content)\n",
    "    \n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to develop a class - image generator\n",
    "\n",
    "- Scrape only scrapable images\n",
    "    - Predictable scrapes: e.g. imgur/reddit\n",
    "        - Reddit API provides a thumbnail!! - Great for speed-up of image processing and IO\n",
    "    - Unpredictable scrapes: Other sites/reddit comments\n",
    "- Develop a pre-buffer\n",
    "    - Image pops are quick, but buffer can buffer in the background\n",
    "    - Store image in buffer folder\n",
    "    > 1 - URL Buffer\n",
    "    \n",
    "    > 2 - Image Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _img_url_extract(img_sub):\n",
    "    \"\"\"\n",
    "    Extract and return the url hyperlink of only PRAW's submission-type objects.\n",
    "    Include the link to a thumbnail if it exists\n",
    "    \"\"\"\n",
    "    if isinstance(img_sub, praw.models.reddit.submission.Submission):\n",
    "        return (img_sub.url, img_sub.thumbnail)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_submission(img_sub):\n",
    "    return isinstance(img_sub, praw.models.reddit.submission.Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_img(url):\n",
    "    return url.split('.')[-1] in {'jpg','png'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_imgur(url):\n",
    "    return 'imgur' in url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgur_oath = {\n",
    "    'client_id': '29b27fc1363aa92',\n",
    "    'client_secret': 'ef58b651b764dfb487809e64a2d62982e1a6392e'\n",
    "}\n",
    "\n",
    "imgur_headers = {\n",
    "    'Authorization': f'Client-ID {imgur_oath[\"client_id\"]}'\n",
    "}\n",
    "\n",
    "def get_imgur_img(url):\n",
    "    img_urls = []\n",
    "    imgur_hash = url.strip('/').split('/')[-1].split('?')[0]\n",
    "    \n",
    "    # Try album endpoint API\n",
    "    api_url = f'https://api.imgur.com/3/album/{imgur_hash}/images'\n",
    "    response = requests.request('GET', api_url, headers = imgur_headers, allow_redirects=False)\n",
    "    if response.ok:\n",
    "        img_urls += [(image['link'], None) for image in json.loads(response.text)['data']]\n",
    "    \n",
    "    else:\n",
    "        # Try gallery endpoint API\n",
    "        api_url = f'https://api.imgur.com/3/gallery/album/{gallery_hash}'\n",
    "        response = requests.request('GET', api_url, headers = imgur_headers, allow_redirects=False)\n",
    "        if response.ok:\n",
    "            img_urls += [(image['link'], None) for image in json.loads(response.text)['data']['images']]\n",
    "\n",
    "    if len(img_urls) == 0:\n",
    "        print(img_url, 'ERROROROROROROROROROROR')\n",
    "        \n",
    "    return img_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imgur_img('https://imgur.com/a/WepxW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imgur_img('https://imgur.com/a/rSZlZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imgur_img('https://imgur.com/a/22D4xOJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WallpaperScraper('wallpapers', 'top', 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "urls = []\n",
    "non_reddit = 0\n",
    "for idx, img_sub in enumerate(test._wallpapers):\n",
    "    if is_submission(img_sub):\n",
    "        if is_img(img_sub.url):\n",
    "            urls.append((img_sub.url, img_sub.thumbnail))\n",
    "        elif is_imgur(img_sub.url):\n",
    "            urls += get_imgur_img(img_sub.url)\n",
    "        else:\n",
    "            non_reddit += 1\n",
    "    \n",
    "    if idx == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.submission import Submission\n",
    "\n",
    "class WallpaperScraper:\n",
    "    \"\"\"\n",
    "    Initializes and creates an instance of scraped submissions to the /r/subreddit.\n",
    "    Stores submissions in the instance's 'wallpapers' attribute.\n",
    "    \"\"\"\n",
    "    \n",
    "    reddit_oath = {\n",
    "        'client_id': 'pIxpnAoiGfwE-g',\n",
    "        'client_secret': 'Ueccpv4dJegXUYbmAIdwxevxjDs',\n",
    "        'user_agent': 'wallpapers'\n",
    "    }\n",
    "    \n",
    "    imgur_oath = {\n",
    "        'client_id': '29b27fc1363aa92',\n",
    "        'client_secret': 'ef58b651b764dfb487809e64a2d62982e1a6392e'\n",
    "    }\n",
    "    \n",
    "    imgur_headers = {\n",
    "      'Authorization': f'Client-ID {imgur_oath[\"client_id\"]}'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def __init__(self, sort='top', time_filter='week', limit=300):\n",
    "        \"\"\"\n",
    "        Initialize a wallpaper subreddit scraper instance with feed options.\n",
    "        Stores wallpaper urls in a 'wallpaper' instance attribute'.\n",
    "        \n",
    "        Sort options: 'top' (default), 'controversial', 'gilded', 'hot', 'new', 'rising'\n",
    "        Time filters: 'week' (default), 'all', 'day', 'hour', 'month', 'year'\n",
    "        Limit: Scrapes up to 1000 (max) submissions, 300 by default\n",
    "        \"\"\"\n",
    "        self.wallpapers = []\n",
    "        \n",
    "        with Reddit(**self.reddit_oath) as R:\n",
    "            r_wallpapers = R.subreddit('wallpapers')\n",
    "            \n",
    "            # sort_options = {'controversial', 'gilded', 'hot', 'new', 'rising', 'top'}\n",
    "            # time_filter = {'all', 'day', 'hour', 'month', 'week', 'year'}\n",
    "            \n",
    "            if sort in {'controversial', 'top'}:\n",
    "                r_wallpapers = getattr(r_wallpapers, sort)(limit=limit, time_filter=time_filter)\n",
    "            else:\n",
    "                r_wallpapers = getattr(r_wallpapers, sort)(limit=limit)\n",
    "                \n",
    "            for submission in r_wallpapers:\n",
    "                self._get_wallpaper_url(submission)\n",
    "                \n",
    "                \n",
    "    def __iter__(self):\n",
    "        return iter(self.wallpapers)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_imgur(url):\n",
    "        \"\"\"\n",
    "        Queries the Imgur API with GET requests for images, given a valid Imgur url.\n",
    "        This method is called to extract image urls from albums/galleries of images.\n",
    "        \"\"\"\n",
    "        img_urls = []\n",
    "        imgur_hash = url.strip('/').split('/')[-1].split('?')[0]\n",
    "        \n",
    "        # Unpredictable API endpoint\n",
    "        api_album = f'https://api.imgur.com/3/album/{imgur_hash}/images'\n",
    "        api_gallery = f'https://api.imgur.com/3/gallery/album/{imgur_hash}'\n",
    "        api_kwargs = {\n",
    "            'method':'GET',\n",
    "            'url': None,\n",
    "            'headers': WallpaperScraper.imgur_headers,\n",
    "            'allow_redirects': False\n",
    "        }\n",
    "        \n",
    "        for endpoint in [api_album, api_gallery]:\n",
    "            api_kwargs['url'] = endpoint\n",
    "            response = requests.request(**api_kwargs)\n",
    "            if response.ok:\n",
    "                # Imgur is very unpredictable!\n",
    "                try:\n",
    "                    img_urls += [image['link'] for image in json.loads(response.text)['data']]\n",
    "                except TypeError:\n",
    "                    img_urls += [image['link'] for image in json.loads(response.text)['data']['images']]\n",
    "                break       \n",
    "        return img_urls\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_img_ext(url):\n",
    "        \"\"\"\n",
    "        Determines existence of a .jpg or .png file extension from a url string\n",
    "        \"\"\"\n",
    "        return url[-4:] in {'.jpg','.png'} \n",
    "\n",
    "    \n",
    "    def _get_wallpaper_url(self, img_sub):\n",
    "        \"\"\"\n",
    "        Extract wallpaper url's from reddit submissions/posts. Appends to the instance's\n",
    "        'wallpaper' attribute.\n",
    "        \"\"\"\n",
    "        if isinstance(img_sub, Submission):\n",
    "            if self.is_img_ext(img_sub.url):\n",
    "                self.wallpapers.append((img_sub.url, img_sub.thumbnail))\n",
    "            elif 'imgur' in img_sub.url[:15]: # No need to search entire string\n",
    "                self.wallpapers.extend([(img_url, None) for img_url in self.get_imgur(img_sub.url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 s ± 168 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "WallpaperScraper('top', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WallpaperScraper('top', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.wallpapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://i.imgur.com/5yeBVeM.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/z68RM_D9H4pQ2BEV7QSETAU5XMZ6ruILkGeCYAH_m6A.jpg'),\n",
       " ('http://i.imgur.com/Z6kdWmA.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/yuNAitd2vU4Hv3YLbWeSbec663RW6IowhvoUYKrO7CU.jpg'),\n",
       " ('http://i.imgur.com/ezOgN0q.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/tXYU39sw5Fh34EZuaJrL6w24RFHs94xcohm1Mdx5QBc.jpg'),\n",
       " ('http://i.imgur.com/X39qMYI.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/x9wjjWOuxx65dsYI3U7WqPxGiTCk3ZDM2OwUSr9WhlM.jpg'),\n",
       " ('https://i.redd.it/gy65o4mk3oe01.png',\n",
       "  'https://b.thumbs.redditmedia.com/KrPbXIcuyyk68qOuJbHCd9X9cuHeytNeBXqnt5H0Oxc.jpg'),\n",
       " ('https://i.imgur.com/CUGGaiZ.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/Kw0ZGoVkAdy_w2SWp4Al8GMivRZqyqLPW0O3BMrQIaU.jpg'),\n",
       " ('https://i.redd.it/n1ci0trfgrey.jpg',\n",
       "  'https://a.thumbs.redditmedia.com/vCbBJmROViB7huvRz6foyznVN4Cpc9W1YwXWSp5z5e8.jpg'),\n",
       " ('https://i.redd.it/8kd8jy4vnem01.png',\n",
       "  'https://b.thumbs.redditmedia.com/XZdg0AGYvyxs-W5iaVKSjEhB9RDDS39Rxy76329JLMk.jpg'),\n",
       " ('http://i.imgur.com/Fx6BGlt.jpg',\n",
       "  'https://a.thumbs.redditmedia.com/_0zkd3wibjGDhCrqRdeftHGeQdnCNKNFfax9FaMPxj4.jpg'),\n",
       " ('http://i.imgur.com/OUe6WiC.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/uNg7tId1oLrT8DehYOnRbOaW0IAuGrLNu7pdDkECguo.jpg')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.wallpapers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://i.imgur.com/Z6kdWmA.jpg',\n",
       " 'https://b.thumbs.redditmedia.com/yuNAitd2vU4Hv3YLbWeSbec663RW6IowhvoUYKrO7CU.jpg')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
