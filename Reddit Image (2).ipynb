{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New notebook to keep dev clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import praw\n",
    "from praw import Reddit\n",
    "\n",
    "class WallpaperScraper:\n",
    "    reddit_oath = {\n",
    "        'client_id': 'pIxpnAoiGfwE-g',\n",
    "        'client_secret': 'Ueccpv4dJegXUYbmAIdwxevxjDs',\n",
    "        'user_agent': 'wallpapers'\n",
    "    }\n",
    "    \n",
    "    imgur_oath = {\n",
    "        'client_id': '29b27fc1363aa92',\n",
    "        'client_secret': 'ef58b651b764dfb487809e64a2d62982e1a6392e'\n",
    "    }\n",
    "    \n",
    "    imgur_headers = {\n",
    "      'Authorization': f'Client-ID {imgur_oath[\"client_id\"]}'\n",
    "    }\n",
    "    \n",
    "#     sort_options = {'controversial', 'gilded', 'hot', 'new', 'rising', 'top'}\n",
    "#     time_filter = {'all', 'day', 'hour', 'month', 'week', 'year'}\n",
    "    \n",
    "    def __init__(self, img_dir, sort='top', time_filter='week'):\n",
    "        \"\"\"\n",
    "        Initialize a wallpaper subreddit scraper instance with feed options.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._img_dir = img_dir\n",
    "        self._mk_img_dir()\n",
    "        \n",
    "        self._wallpapers = Reddit(**self.reddit_oath).subreddit('wallpapers')\n",
    "        if sort in {'controversial', 'top'}:\n",
    "            self._wallpapers = getattr(self._wallpapers, sort)(limit=1000, time_filter=time_filter)\n",
    "        else:\n",
    "            self._wallpapers = getattr(self._wallpapers, sort)(limit=1000)\n",
    "        \n",
    "    def _mk_img_dir(self):\n",
    "        \"\"\"\n",
    "        Checks and initializes a directory for image files to be downloaded.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self._img_dir):\n",
    "            os.mkdir(self._img_dir)\n",
    "        if not os.path.exists(os.path.join(self._img_dir, 'buffer')):\n",
    "            os.mkdir(os.path.join(self._img_dir, 'buffer'))\n",
    "            \n",
    "    def _img_url_extract(self, img_sub):\n",
    "        \"\"\"\n",
    "        Extract and return the url hyperlink of only PRAW's submission-type objects.\n",
    "        \"\"\"\n",
    "        if isinstance(img_sub, praw.models.reddit.submission.Submission):\n",
    "            return img_sub.url\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = Reddit(**WallpaperScraper.reddit_oath).subreddit('wallpapers')\n",
    "\n",
    "# Time filters only allowed for 'controversial' and 'top' sort orders\n",
    "\n",
    "for i in ['controversial', 'gilded', 'hot', 'new', 'rising', 'top']:\n",
    "    for j in ['all', 'day', 'hour', 'month', 'week', 'year']:\n",
    "        try:\n",
    "            getattr(test, i)(time_filter=j)\n",
    "        except:\n",
    "            print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WallpaperScraper('wallpapers', 'top', 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, img_sub in enumerate(test._wallpapers):\n",
    "    if isinstance(img_sub, praw.models.reddit.submission.Submission):\n",
    "        img_sub.url.split('.')[-1]\n",
    "        file_ext = img_sub.url.split('.')[-1]\n",
    "        print(img_sub.url, file_ext, img_sub.thumbnail)\n",
    "    #     with open(f'tmp/tmp{idx}.{file_ext}', 'wb') as tmp:\n",
    "    #         r = requests.get(img_sub.url, stream=True)\n",
    "    #         if r.ok:\n",
    "    #             tmp.write(r.content)\n",
    "    #     file_ext = img_sub.thumbnail.split('.')[-1]\n",
    "    #     with open(f'tmp/tmp{idx}_.{file_ext}', 'wb') as tmp:\n",
    "    #         r = requests.get(img_sub.thumbnail, stream=True)\n",
    "    #         if r.ok:\n",
    "    #             tmp.write(r.content)\n",
    "    \n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to develop a class - image generator\n",
    "\n",
    "- Scrape only scrapable images\n",
    "    - Predictable scrapes: e.g. imgur/reddit\n",
    "        - Reddit API provides a thumbnail!! - Great for speed-up of image processing and IO\n",
    "    - Unpredictable scrapes: Other sites/reddit comments\n",
    "- Develop a pre-buffer\n",
    "    - Image pops are quick, but buffer can buffer in the background\n",
    "    - Store image in buffer folder\n",
    "    > 1 - URL Buffer\n",
    "    \n",
    "    > 2 - Image Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _img_url_extract(img_sub):\n",
    "    \"\"\"\n",
    "    Extract and return the url hyperlink of only PRAW's submission-type objects.\n",
    "    Include the link to a thumbnail if it exists\n",
    "    \"\"\"\n",
    "    if isinstance(img_sub, praw.models.reddit.submission.Submission):\n",
    "        return (img_sub.url, img_sub.thumbnail)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_submission(img_sub):\n",
    "    return isinstance(img_sub, praw.models.reddit.submission.Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_img(url):\n",
    "    return url.split('.')[-1] in {'jpg','png'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_imgur(url):\n",
    "    return 'imgur' in url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgur_oath = {\n",
    "    'client_id': '29b27fc1363aa92',\n",
    "    'client_secret': 'ef58b651b764dfb487809e64a2d62982e1a6392e'\n",
    "}\n",
    "\n",
    "imgur_headers = {\n",
    "    'Authorization': f'Client-ID {imgur_oath[\"client_id\"]}'\n",
    "}\n",
    "\n",
    "def get_imgur_img(url):\n",
    "    img_urls = []\n",
    "    imgur_hash = url.strip('/').split('/')[-1].split('?')[0]\n",
    "    \n",
    "    # Try album endpoint API\n",
    "    api_url = f'https://api.imgur.com/3/album/{imgur_hash}/images'\n",
    "    response = requests.request('GET', api_url, headers = imgur_headers, allow_redirects=False)\n",
    "    if response.ok:\n",
    "        img_urls += [(image['link'], None) for image in json.loads(response.text)['data']]\n",
    "    \n",
    "    else:\n",
    "        # Try gallery endpoint API\n",
    "        api_url = f'https://api.imgur.com/3/gallery/album/{gallery_hash}'\n",
    "        response = requests.request('GET', api_url, headers = imgur_headers, allow_redirects=False)\n",
    "        if response.ok:\n",
    "            img_urls += [(image['link'], None) for image in json.loads(response.text)['data']['images']]\n",
    "\n",
    "    if len(img_urls) == 0:\n",
    "        print(img_url, 'ERROROROROROROROROROROR')\n",
    "        \n",
    "    return img_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imgur_img('https://imgur.com/a/WepxW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imgur_img('https://imgur.com/a/rSZlZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imgur_img('https://imgur.com/a/22D4xOJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WallpaperScraper('wallpapers', 'top', 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "urls = []\n",
    "non_reddit = 0\n",
    "for idx, img_sub in enumerate(test._wallpapers):\n",
    "    if is_submission(img_sub):\n",
    "        if is_img(img_sub.url):\n",
    "            urls.append((img_sub.url, img_sub.thumbnail))\n",
    "        elif is_imgur(img_sub.url):\n",
    "            urls += get_imgur_img(img_sub.url)\n",
    "        else:\n",
    "            non_reddit += 1\n",
    "    \n",
    "    if idx == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Queue().qsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.submission import Submission\n",
    "\n",
    "\n",
    "def is_img_ext(file):\n",
    "    \"\"\"\n",
    "    Determines existence of a .jpg or .png file extension from a url string\n",
    "    \"\"\"\n",
    "    return file[-4:] in {'.jpg','.png'} \n",
    "\n",
    "\n",
    "class RWallpaperScraper:\n",
    "    \"\"\"\n",
    "    Initializes and creates an instance of scraped submissions to the /r/subreddit.\n",
    "    Stores submissions in the instance's 'wallpapers' attribute.\n",
    "    \"\"\"\n",
    "    \n",
    "    reddit_oath = {\n",
    "        'client_id': 'pIxpnAoiGfwE-g',\n",
    "        'client_secret': 'Ueccpv4dJegXUYbmAIdwxevxjDs',\n",
    "        'user_agent': 'wallpapers'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    imgur_oath = {\n",
    "        'client_id': '29b27fc1363aa92',\n",
    "        'client_secret': 'ef58b651b764dfb487809e64a2d62982e1a6392e'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    imgur_headers = {\n",
    "      'Authorization': f'Client-ID {imgur_oath[\"client_id\"]}'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_imgur(url):\n",
    "        \"\"\"\n",
    "        Queries the Imgur API with GET requests for images, given a valid Imgur url.\n",
    "        This method is called to extract image urls from albums/galleries of images.\n",
    "        \"\"\"\n",
    "        img_urls = []\n",
    "        imgur_hash = url.strip('/').split('/')[-1].split('?')[0]\n",
    "        \n",
    "        # Unpredictable API endpoint\n",
    "        api_album = f'https://api.imgur.com/3/album/{imgur_hash}/images'\n",
    "        api_gallery = f'https://api.imgur.com/3/gallery/album/{imgur_hash}'\n",
    "        api_kwargs = {\n",
    "            'method':'GET',\n",
    "            'url': None,\n",
    "            'headers': RWallpaperScraper.imgur_headers,\n",
    "            'allow_redirects': False\n",
    "        }\n",
    "        \n",
    "        for endpoint in [api_album, api_gallery]:\n",
    "            api_kwargs['url'] = endpoint\n",
    "            response = requests.request(**api_kwargs)\n",
    "            if response.ok:\n",
    "                # Imgur is very unpredictable!\n",
    "                try:\n",
    "                    img_urls += [image['link'] for image in json.loads(response.text)['data']]\n",
    "                except TypeError:\n",
    "                    img_urls += [image['link'] for image in json.loads(response.text)['data']['images']]\n",
    "                break       \n",
    "        return img_urls\n",
    "    \n",
    "    \n",
    "    def __init__(self, sort='top', time_filter='week', limit=300):\n",
    "        \"\"\"\n",
    "        Initialize a wallpaper subreddit scraper instance with feed options.\n",
    "        Stores wallpaper urls in a 'wallpaper' instance attribute'.\n",
    "        \n",
    "        Sort options: 'top' (default), 'controversial', 'gilded', 'hot', 'new', 'rising'\n",
    "        Time filters: 'week' (default), 'all', 'day', 'hour', 'month', 'year'\n",
    "        Limit: Scrapes up to 1000 (max) submissions, 300 by default\n",
    "        \"\"\"\n",
    "        self.wallpapers = []\n",
    "        \n",
    "        with Reddit(**self.reddit_oath) as R:\n",
    "            r_wallpapers = R.subreddit('wallpapers')\n",
    "            \n",
    "            # sort_options = {'controversial', 'gilded', 'hot', 'new', 'rising', 'top'}\n",
    "            # time_filter = {'all', 'day', 'hour', 'month', 'week', 'year'}\n",
    "            \n",
    "            if sort in {'controversial', 'top'}:\n",
    "                r_wallpapers = getattr(r_wallpapers, sort)(limit=limit, time_filter=time_filter)\n",
    "            else:\n",
    "                r_wallpapers = getattr(r_wallpapers, sort)(limit=limit)\n",
    "            \n",
    "            self._get_wallpaper_url_threaded(r_wallpapers)\n",
    "            sleep(3)\n",
    "            print(self.wallpapers[:25])\n",
    "                \n",
    "                \n",
    "    def __iter__(self):\n",
    "        return iter(self.wallpapers)\n",
    "\n",
    "    \n",
    "    def _get_wallpaper_url(self, img_sub):\n",
    "        \"\"\"\n",
    "        Extract wallpaper url's from reddit submissions/posts. Appends to the instance's\n",
    "        'wallpaper' attribute.\n",
    "        \"\"\"\n",
    "        if isinstance(img_sub, Submission):\n",
    "            if is_img_ext(img_sub.url):\n",
    "                self.wallpapers.append((img_sub.url, img_sub.thumbnail))\n",
    "            elif 'imgur' in img_sub.url[:15]: # No need to search entire string\n",
    "                self.wallpapers.extend([(img_url, None) for img_url in self.get_imgur(img_sub.url)])\n",
    "    \n",
    "    \n",
    "    def _get_wallpaper_url_threaded(self, r_wallpapers, thread_point=50):\n",
    "        if thread_point:\n",
    "            loops = 0\n",
    "            for submission in r_wallpapers:\n",
    "                self._get_wallpaper_url(submission)\n",
    "                loops += 1\n",
    "                if loops == thread_point:\n",
    "                    break\n",
    "        else:\n",
    "            for submission in r_wallpapers:\n",
    "                self._get_wallpaper_url(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://i.redd.it/0as8pvk6m8z31.jpg', 'https://b.thumbs.redditmedia.com/EuqWqTtOMpIKNSDIYebvKTFitwoxsP5p4LjUhVD96rc.jpg'), ('https://i.redd.it/ia4x8geen2z31.jpg', 'https://b.thumbs.redditmedia.com/gxtKJcdVPyJ3Q_1dP8_77zl8KBfW3nMq-ptoJSVt2gQ.jpg'), ('https://i.redd.it/k30zvw2c2fz31.png', 'https://b.thumbs.redditmedia.com/CV5fRydXqwWlmgB2c92X6zDsR9Z745pH1kZBcLA7uoQ.jpg'), ('https://i.redd.it/lia98qq9wyz31.jpg', 'https://b.thumbs.redditmedia.com/A1mnxoMvsMzGzMznJeEpbG2STrNX0-5RFRJWBgWLtSU.jpg'), ('https://i.redd.it/oybtdwhu93041.jpg', 'https://a.thumbs.redditmedia.com/q-Dr67N8OmWlrws9heUm3-cKW8oAkJzcTZO4oucFbc4.jpg'), ('https://i.redd.it/hhnfd8u8qtz31.jpg', 'https://b.thumbs.redditmedia.com/nFl0tzTOSwUr9R_E34nBlCtcdvQvO4_7x5ZZory0Uyk.jpg'), ('https://i.redd.it/xud2hf94znz31.jpg', 'https://b.thumbs.redditmedia.com/kz35OZDQLPEY7N090UItowwt86JXeZP5DUdtQw628CU.jpg'), ('https://i.redd.it/jf91nwtl2zy31.jpg', 'https://b.thumbs.redditmedia.com/vSq4t7tKs8ImFYtKxN4Sdm0i8gZJXdXVi1z6F2gjDyM.jpg'), ('https://i.redd.it/ue05165ttiz31.png', 'https://b.thumbs.redditmedia.com/couQVJ0dMB0S6I7qmZKTUbqB85hrKQUHzuQA5Rtd1SE.jpg'), ('https://i.redd.it/chyuh626jdz31.jpg', 'https://b.thumbs.redditmedia.com/x1IYbazOw1H_DzVkrDUM33vaj3BIdmRn-Eex5pWfT4Y.jpg'), ('https://i.redd.it/cz1mby87v8041.png', 'https://b.thumbs.redditmedia.com/bJmlTK0rzFRe3ZJCZZkOw6O-CiufsRcWAcXlo52dGzA.jpg'), ('https://i.redd.it/jf7zqjplhiz31.png', 'https://b.thumbs.redditmedia.com/XTL4e3CpsYFH_MzdxC94eLNru2D1XwbxqSvpKRQCU7k.jpg'), ('https://i.redd.it/gmpl3v9t1zy31.jpg', 'https://b.thumbs.redditmedia.com/C7SC6gamLsJj7gRWXEDttoFqHWN4QMl4VYTkcmybZgU.jpg'), ('https://i.redd.it/q4yhjpiudrz31.jpg', 'https://b.thumbs.redditmedia.com/GZ-uhJGGnaf2_mr8BslPOROBBl2RwvX505YM76S95TQ.jpg'), ('https://i.redd.it/ii3r909y7qz31.png', 'https://b.thumbs.redditmedia.com/ZuRTVT652z3clZFZ53qBa8xD3hvDJKh_Sz72e5cL2HM.jpg'), ('https://i.imgur.com/PD676w7.png', 'https://a.thumbs.redditmedia.com/CTdCdH3CaqTmV_q4GYw0tfU-VevX1ArPGz9B9Vc-794.jpg'), ('https://i.redd.it/lcl5lr1qnez31.jpg', 'https://b.thumbs.redditmedia.com/IR1IL3YJ8-bPzN_xOhWUgQNulR_U5krhDAPhdFOAtTU.jpg'), ('https://i.redd.it/kdjq5eukjtz31.jpg', 'https://b.thumbs.redditmedia.com/XSRT8nji47PGSl8VCvz_tuh-9bMIg6GMbJy45xuRpIk.jpg'), ('https://i.imgur.com/ZDhxOOv.jpg', 'https://a.thumbs.redditmedia.com/gn2vZVyZovfT83C_EPC7lax3RYHAiSG0Tcyp7t8Sj60.jpg'), ('https://i.redd.it/paeewlo73xz31.jpg', 'https://b.thumbs.redditmedia.com/GUoIuhq3LcI9bP8y76aWimUtvkgrgmfj6Vjq7JPw90Q.jpg'), ('https://i.redd.it/pl62vfcqj2041.jpg', 'https://b.thumbs.redditmedia.com/TnzYydMu_2Si29XCXdcyaClRGETPr8bNVuojLn_5WtY.jpg'), ('https://i.redd.it/onps576m8jz31.jpg', 'https://b.thumbs.redditmedia.com/j46uafsZGfk7O6THvWUwTikfcQsJEAnOhCeVE80DMus.jpg'), ('https://i.redd.it/gjj1kfujhcz31.jpg', 'https://b.thumbs.redditmedia.com/BMSsSEnvN894Bp0QU8NqNlzDT32MbupFpYfTT6Od-Wg.jpg'), ('https://i.redd.it/tq7xlx2q83041.png', 'https://b.thumbs.redditmedia.com/GqORztmnd83fNS4xfjqHET2G70VU7sLNArO1Rs4ukYM.jpg'), ('https://i.redd.it/op4rde44d8z31.jpg', 'https://b.thumbs.redditmedia.com/jLpJvRbpXJe5FwwiBDO3IZxzop8BfEEe4D92-sDCqjQ.jpg')]\n"
     ]
    }
   ],
   "source": [
    "test = RWallpaperScraper('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.wallpapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def looper(loops):\n",
    "    \n",
    "    def gen(*args, **kwargs):\n",
    "\n",
    "        func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxlspp\n",
      "dx8oj3\n",
      "dy1594\n",
      "\n",
      "dzdzdg\n",
      "dzo8jn\n",
      "dz0mo8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RWallpaperScraper at 0x122f54890>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RWallpaperScraper('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "thread done\n",
      "[0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "testing....\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "def sleep_some(secs):\n",
    "    sleep(secs)\n",
    "    print(f'slept {secs} seconds')\n",
    "    \n",
    "    \n",
    "def threaded_array(A):\n",
    "    sleep(2)\n",
    "    for i in range(10):\n",
    "        sleep(1)\n",
    "        A.append(i)\n",
    "\n",
    "def try_thread():\n",
    "#     i = randint(1,10)\n",
    "    i = 10\n",
    "    A = [*range(5)]\n",
    "    \n",
    "#     print(i, 'seconds')\n",
    "    t = Thread(target=threaded_array, args=(A,))\n",
    "\n",
    "    t.start()\n",
    "    \n",
    "    print(A[:10])\n",
    "    sleep(4)\n",
    "    \n",
    "    t.join()\n",
    "    print('thread done')\n",
    "    print(A)\n",
    "    \n",
    "\n",
    "try_thread()\n",
    "print('testing....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queue.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "we running\n",
      "0\n",
      "0\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "test = RWallpaperScraper('top', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.wallpapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://i.imgur.com/5yeBVeM.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/z68RM_D9H4pQ2BEV7QSETAU5XMZ6ruILkGeCYAH_m6A.jpg'),\n",
       " ('http://i.imgur.com/Z6kdWmA.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/yuNAitd2vU4Hv3YLbWeSbec663RW6IowhvoUYKrO7CU.jpg'),\n",
       " ('http://i.imgur.com/ezOgN0q.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/tXYU39sw5Fh34EZuaJrL6w24RFHs94xcohm1Mdx5QBc.jpg'),\n",
       " ('http://i.imgur.com/X39qMYI.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/x9wjjWOuxx65dsYI3U7WqPxGiTCk3ZDM2OwUSr9WhlM.jpg'),\n",
       " ('https://i.redd.it/gy65o4mk3oe01.png',\n",
       "  'https://b.thumbs.redditmedia.com/KrPbXIcuyyk68qOuJbHCd9X9cuHeytNeBXqnt5H0Oxc.jpg'),\n",
       " ('https://i.imgur.com/CUGGaiZ.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/Kw0ZGoVkAdy_w2SWp4Al8GMivRZqyqLPW0O3BMrQIaU.jpg'),\n",
       " ('https://i.redd.it/n1ci0trfgrey.jpg',\n",
       "  'https://a.thumbs.redditmedia.com/vCbBJmROViB7huvRz6foyznVN4Cpc9W1YwXWSp5z5e8.jpg'),\n",
       " ('https://i.redd.it/8kd8jy4vnem01.png',\n",
       "  'https://b.thumbs.redditmedia.com/XZdg0AGYvyxs-W5iaVKSjEhB9RDDS39Rxy76329JLMk.jpg'),\n",
       " ('http://i.imgur.com/Fx6BGlt.jpg',\n",
       "  'https://a.thumbs.redditmedia.com/_0zkd3wibjGDhCrqRdeftHGeQdnCNKNFfax9FaMPxj4.jpg'),\n",
       " ('http://i.imgur.com/OUe6WiC.jpg',\n",
       "  'https://b.thumbs.redditmedia.com/uNg7tId1oLrT8DehYOnRbOaW0IAuGrLNu7pdDkECguo.jpg')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.wallpapers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://i.imgur.com/Z6kdWmA.jpg',\n",
       " 'https://b.thumbs.redditmedia.com/yuNAitd2vU4Hv3YLbWeSbec663RW6IowhvoUYKrO7CU.jpg')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to speed up and allow threading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.submission import Submission\n",
    "\n",
    "\n",
    "def is_img_ext(file):\n",
    "    \"\"\"\n",
    "    Determines existence of a .jpg or .png file extension from a url string\n",
    "    \"\"\"\n",
    "    return file[-4:] in {'.jpg','.png'} \n",
    "\n",
    "\n",
    "class RWallpaperScraper:\n",
    "    \"\"\"\n",
    "    Initializes and creates an instance of scraped submissions to the /r/subreddit.\n",
    "    Stores submissions in the instance's 'wallpapers' attribute.\n",
    "    \"\"\"\n",
    "    \n",
    "    reddit_oath = {\n",
    "        'client_id': 'pIxpnAoiGfwE-g',\n",
    "        'client_secret': 'Ueccpv4dJegXUYbmAIdwxevxjDs',\n",
    "        'user_agent': 'wallpapers'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    imgur_oath = {\n",
    "        'client_id': '29b27fc1363aa92',\n",
    "        'client_secret': 'ef58b651b764dfb487809e64a2d62982e1a6392e'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    imgur_headers = {\n",
    "      'Authorization': f'Client-ID {imgur_oath[\"client_id\"]}'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_imgur(url):\n",
    "        \"\"\"\n",
    "        Queries the Imgur API with GET requests for images, given a valid Imgur url.\n",
    "        This method is called to extract image urls from albums/galleries of images.\n",
    "        \"\"\"\n",
    "        img_urls = []\n",
    "        imgur_hash = url.strip('/').split('/')[-1].split('?')[0]\n",
    "        \n",
    "        # Unpredictable API endpoint\n",
    "        api_album = f'https://api.imgur.com/3/album/{imgur_hash}/images'\n",
    "        api_gallery = f'https://api.imgur.com/3/gallery/album/{imgur_hash}'\n",
    "        api_kwargs = {\n",
    "            'method':'GET',\n",
    "            'url': None,\n",
    "            'headers': RWallpaperScraper.imgur_headers,\n",
    "            'allow_redirects': False\n",
    "        }\n",
    "        \n",
    "        for endpoint in [api_album, api_gallery]:\n",
    "            api_kwargs['url'] = endpoint\n",
    "            response = requests.request(**api_kwargs)\n",
    "            if response.ok:\n",
    "                # Imgur is very unpredictable!\n",
    "                try:\n",
    "                    img_urls += [image['link'] for image in json.loads(response.text)['data']]\n",
    "                except TypeError:\n",
    "                    img_urls += [image['link'] for image in json.loads(response.text)['data']['images']]\n",
    "                break       \n",
    "        return img_urls\n",
    "    \n",
    "    \n",
    "    def __init__(self, sort='top', time_filter='week', limit=300):\n",
    "        \"\"\"\n",
    "        Initialize a wallpaper subreddit scraper instance with feed options.\n",
    "        Stores wallpaper urls in a 'wallpaper' instance attribute'.\n",
    "        \n",
    "        Sort options: 'top' (default), 'controversial', 'gilded', 'hot', 'new', 'rising'\n",
    "        Time filters: 'week' (default), 'all', 'day', 'hour', 'month', 'year'\n",
    "        Limit: Scrapes up to 1000 (max) submissions, 300 by default\n",
    "        \"\"\"\n",
    "        self.wallpapers = []\n",
    "                \n",
    "                \n",
    "    def __iter__(self):\n",
    "        return iter(self.wallpapers)\n",
    "    \n",
    "\n",
    "    def start(self):\n",
    "        with Reddit(**self.reddit_oath) as R:\n",
    "            r_wallpapers = R.subreddit('wallpapers')\n",
    "\n",
    "            # sort_options = {'controversial', 'gilded', 'hot', 'new', 'rising', 'top'}\n",
    "            # time_filter = {'all', 'day', 'hour', 'month', 'week', 'year'}\n",
    "\n",
    "            if sort in {'controversial', 'top'}:\n",
    "                r_wallpapers = getattr(r_wallpapers, sort)(limit=limit, time_filter=time_filter)\n",
    "            else:\n",
    "                r_wallpapers = getattr(r_wallpapers, sort)(limit=limit)\n",
    "\n",
    "            for submission in r_wallpapers:\n",
    "                self._get_wallpaper_url(submission)\n",
    "\n",
    "    \n",
    "    def _get_wallpaper_url(self, img_sub):\n",
    "        \"\"\"\n",
    "        Extract wallpaper url's from reddit submissions/posts. Appends to the instance's\n",
    "        'wallpaper' attribute.\n",
    "        \"\"\"\n",
    "        if isinstance(img_sub, Submission):\n",
    "            if is_img_ext(img_sub.url):\n",
    "                self.wallpapers.append((img_sub.url, img_sub.thumbnail))\n",
    "            elif 'imgur' in img_sub.url[:15]: # No need to search entire string\n",
    "                self.wallpapers.extend([(img_url, None) for img_url in self.get_imgur(img_sub.url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WallpaperScraper('top', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c3c59593b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_img_ext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from . import is_img_ext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
